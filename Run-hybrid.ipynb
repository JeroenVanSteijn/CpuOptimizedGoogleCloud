{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement libcloud (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for libcloud\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install libcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mechanize\n",
    "import numpy\n",
    "import os\n",
    "import queue\n",
    "import random\n",
    "import shutil\n",
    "import socket\n",
    "import string\n",
    "import threading\n",
    "import time\n",
    "import urllib.request\n",
    "from abc import ABC, abstractmethod\n",
    "import pandas\n",
    "import libcloud\n",
    "import paramiko\n",
    "from dataclasses import dataclass\n",
    "from libcloud.compute.providers import get_driver\n",
    "from libcloud.compute.types import Provider\n",
    "from paramiko.buffered_pipe import PipeTimeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSH_USER = 'am72ghiassi'\n",
    "### MARC ###\n",
    "GCLOUD_ACCOUNT = '460631779290-compute@developer.gserviceaccount.com'\n",
    "GCLOUD_KEY_PATH = 'quantative-2-34bd7bef0cbc.json'  # The path to the Service Account Key (a JSON file)\n",
    "GCLOUD_PROJECT = '460631779290'  # GCloud project id\n",
    "\n",
    "### ERWIN ###\n",
    "#GCLOUD_ACCOUNT = '352392662801-compute@developer.gserviceaccount.com'\n",
    "#GCLOUD_KEY_PATH = 'projectgroup-1-da6053635a93.json'\n",
    "#GCLOUD_PROJECT = '352392662801'\n",
    "###\n",
    "\n",
    "DESIGN_CSV = 'design_final.csv'  # The CSV with the experiment design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment ID: alqxmewb\n"
     ]
    }
   ],
   "source": [
    "ComputeEngine = get_driver(Provider.GCE)\n",
    "\n",
    "driver = ComputeEngine(GCLOUD_ACCOUNT, GCLOUD_KEY_PATH, project=GCLOUD_PROJECT)\n",
    "location = [l for l in driver.list_locations() if l.id == '2000'][0]\n",
    "#network = [n for n in driver.ex_list_networks() if n.id ==\n",
    "#           '6096184313360012863'][0]\n",
    "\n",
    "try:\n",
    "    # To prevent problems with GCloud reusing IP addresses\n",
    "    os.remove(f'/home/{SSH_USER}/.ssh/known_hosts')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "ex_id = 'alqxmewb'#''.join(random.choice(string.ascii_lowercase)\n",
    "        #        for i in range(8))  # Generate a random project id\n",
    "\n",
    "EX_RUNTIME = 300  # seconds\n",
    "print(f\"Experiment ID: {ex_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Node(ABC):\n",
    "    def __init__(self, driver, name, master=False, masterNode=None, index=0):\n",
    "        self.driver = driver\n",
    "        self.name = name\n",
    "        self.machineType = 'n1-standard-1'\n",
    "        if not master:\n",
    "            if index % 2 == 0:\n",
    "                self.machineType = 'n2-standard-4'\n",
    "            else:\n",
    "                self.machineType = 'c2-standard-4'\n",
    "        \n",
    "        if not master and masterNode == None:\n",
    "            raise ValueError(\"Slave nodes need a master\")\n",
    "        self.master = masterNode\n",
    "        \n",
    "        self.node = [node for node in driver.list_nodes() if node.name == name][0]\n",
    "        self.disk = [disk for disk in driver.list_volumes() if disk.name == name]\n",
    "        \n",
    "        #self.disk = self.driver.create_volume(40, f\"boot-{self.name}\", image='opendl-2', location=location)\n",
    "        #self.node = self.driver.create_node(\n",
    "         #   name, self.machineType, None, location=location, ex_boot_disk=self.disk)\n",
    "        self.driver.wait_until_running([self.node])\n",
    "        self.pubip = self.node.public_ips[0]\n",
    "        self.privip = self.node.private_ips[0]\n",
    "        self.connected = False\n",
    "\n",
    "        for i in range(5):  # Try 5 times\n",
    "            try:\n",
    "                self.open_ssh()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                time.sleep(5)\n",
    "        if not self.connected:\n",
    "            raise RuntimeError(f\"Can't connect to node {self.name}\")\n",
    "        self.start_type()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close_ssh()\n",
    "\n",
    "    def open_ssh(self):\n",
    "        self.ssh = paramiko.SSHClient()\n",
    "        self.ssh.load_system_host_keys()\n",
    "        self.ssh.set_missing_host_key_policy(paramiko.WarningPolicy())\n",
    "        k = paramiko.RSAKey.from_private_key_file(\"/home/mdroogh/.ssh/gcp\")\n",
    "        print(self.pubip);\n",
    "        self.ssh.connect(self.pubip, port=22, username=SSH_USER, pkey = k)\n",
    "        self.connected = True\n",
    "\n",
    "    def close_ssh(self):\n",
    "        self.connected = False\n",
    "        self.ssh.close()\n",
    "\n",
    "    @abstractmethod\n",
    "    def start_type(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class JobOptions:\n",
    "    batch_size: int\n",
    "    max_epochs: int\n",
    "\n",
    "\n",
    "class MasterNode(Node):\n",
    "    def start_type(self):\n",
    "        stdin, stdout, stderr = self.ssh.exec_command(f'echo \"SPARK_MASTER_HOST=\\'{self.privip}\\'\" >> /home/{SSH_USER}/bd/spark/conf/spark-env.sh')\n",
    "        if (len(stderr.read()) > 0):\n",
    "            print(stdout.read())\n",
    "            print(stderr.read())\n",
    "\n",
    "        stdin, stdout, stderr = self.ssh.exec_command(\n",
    "            '/home/{SSH_USER}/bd/spark/sbin/start-master.sh')\n",
    "        if (len(stderr.read()) > 0):\n",
    "            print(stdout.read())\n",
    "            print(stderr.read())\n",
    "\n",
    "    def submit(self, options: JobOptions):\n",
    "\n",
    "        stdin, stdout, stderr = self.ssh.exec_command(f\"/home/{SSH_USER}/bd/spark/bin/spark-submit --master {self.privip} --driver-cores 1 \"\n",
    "                                                      f\"--driver-memory 1G --total-executor-cores 1 --executor-cores 1 --executor-memory 1G \"\n",
    "                                                      f\"--py-files \\\"/home/{SSH_USER}/bd/spark/lib/bigdl-0.8.0-python-api,/home/{SSH_USER}/bd/codes/bi-rnn.py\\\" \"\n",
    "                                                      f\"--properties-file \\\"/home/{SSH_USER}/bd/spark/conf/spark-bigdl.conf\\\" \"\n",
    "                                                      f\"--jars \\\"/home/{SSH_USER}/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar\\\" \"\n",
    "                                                      f\"--conf \\\"spark.driver.extraClassPath=/home/{SSH_USER}/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar\\\" \"\n",
    "                                                      f\"--conf \\\"spark.executer.extraClassPath=bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar /home/{SSH_USER}/bd/codes/bi-rnn.py\\\" \"\n",
    "                                                      f\"--action train --dataPath \\\"/tmp/mnist\\\" --batchSize {options.batch_size} --endTriggerNum {options.max_epochs} \"\n",
    "                                                      f\"--learningRate 0.01 --learningrateDecay 0.0002\")\n",
    "        if (len(stderr.read()) > 0):\n",
    "            print(stdout.read())\n",
    "            print(stderr.read())\n",
    "\n",
    "    def cancel(self):\n",
    "        br = mechanize.Browser()\n",
    "        br.open(f\"http://{self.pubip}:8080\")\n",
    "\n",
    "        def select_form(form):\n",
    "            return form.attrs.get('action', None) == 'app/kill/'\n",
    "        try:\n",
    "            br.select_form(predicate=select_form)\n",
    "        except mechanize._mechanize.FormNotFoundError:\n",
    "            print(\"FormNotFoundError\")\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred during cancelloing.\")\n",
    "            print(e)\n",
    "        br.submit()\n",
    "\n",
    "\n",
    "class SlaveNode(Node):\n",
    "    def start_type(self):\n",
    "        stdin, stdout, stderr = self.ssh.exec_command(f'/home/{SSH_USER}/bd/spark/sbin/start-slave.sh spark://{self.master.privip}:7077')\n",
    "        if (len(stderr.read()) > 0):\n",
    "            print(stdout.read())\n",
    "            print(stderr.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.194.2.213\n",
      "b''\n",
      "b''\n"
     ]
    }
   ],
   "source": [
    "master = MasterNode(driver, f\"master-{ex_id}-1\", master=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Index  batch_size  max_epochs  learning_rate  memory_size      model  \\\n",
      "0       0         152           3          0.010        12000  lenet5.py   \n",
      "1       1         152           3          0.010        12000  bi-rnn.py   \n",
      "2       2         184           4          0.070        12000  lenet5.py   \n",
      "3       3         184           4          0.070        12000  bi-rnn.py   \n",
      "4       4         112          17          0.005        12000  lenet5.py   \n",
      "..    ...         ...         ...            ...          ...        ...   \n",
      "85     85         128          27          0.020        12000  bi-rnn.py   \n",
      "86     86         192          18          0.010        12000  lenet5.py   \n",
      "87     87         192          18          0.010        12000  bi-rnn.py   \n",
      "88     88         256          12          0.015        12000  lenet5.py   \n",
      "89     89         256          12          0.015        12000  bi-rnn.py   \n",
      "\n",
      "    failure_rate  failure_duration  \n",
      "0              0                 0  \n",
      "1              0                 0  \n",
      "2              0                 0  \n",
      "3              0                 0  \n",
      "4              0                 0  \n",
      "..           ...               ...  \n",
      "85             0                 0  \n",
      "86             0                 0  \n",
      "87             0                 0  \n",
      "88             0                 0  \n",
      "89             0                 0  \n",
      "\n",
      "[90 rows x 8 columns]\n",
      "Number of slaves: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'raw/alqxmewb/design20epochs.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments = pandas.read_csv(DESIGN_CSV)\n",
    "#experiments.columns.values[0] = 'Index'\n",
    "#experiments.set_index('Index')\n",
    "\n",
    "if 'failure_rate' not in experiments:\n",
    "    experiments['failure_rate'] = 0\n",
    "if 'failure_duration' not in experiments:\n",
    "    experiments['failure_duration'] = 0\n",
    "\n",
    "print(experiments)\n",
    "num_slaves = 2\n",
    "print(f\"Number of slaves: {num_slaves}\")\n",
    "\n",
    "os.makedirs(f\"raw/{ex_id}\", exist_ok=True)\n",
    "shutil.copyfile(DESIGN_CSV, f\"raw/{ex_id}/design20epochs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146.148.83.103\n",
      "34.122.7.130\n"
     ]
    }
   ],
   "source": [
    "slaves = [SlaveNode(driver, f\"slave-{ex_id}-{i}\", masterNode=master, index=i) for i in range(1, int(num_slaves)+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 0\n",
      "b''\n",
      "b''\n",
      "Experiment 1\n",
      "b''\n",
      "b''\n",
      "Experiment 2\n",
      "b''\n",
      "b''\n",
      "Experiment 3\n",
      "b''\n",
      "b''\n",
      "Experiment 4\n"
     ]
    }
   ],
   "source": [
    "for idx, row in experiments.iterrows():\n",
    "    print(f\"Experiment {row['Index']}\")\n",
    "    options = JobOptions(int(row['batch_size']), 10)\n",
    "\n",
    "\n",
    "    #for slave in slaves:\n",
    "    #    slave.start_type()\n",
    "\n",
    "\n",
    "    filename = f\"{int(row.Index)}-nodes8-batch{row.batch_size}-epochs{row.max_epochs}-lrate{row.learning_rate}-memory{row.memory_size}-{row.model}.log\"\n",
    "\n",
    "    stdin, stdout, stderr = master.ssh.exec_command(f\"/home/{SSH_USER}/bd/spark/bin/spark-submit --master spark://{master.privip}:7077 --driver-cores 1 \"\n",
    "                                                    f\"--driver-memory 3G --total-executor-cores 8 --executor-cores 4 --executor-memory {int(row.memory_size)}M \"\n",
    "                                                    f\"--py-files /home/{SSH_USER}/bd/spark/lib/bigdl-0.11.0-python-api.zip,/home/{SSH_USER}/bd/codes/{row.model} \"\n",
    "                                                    f\"--properties-file /home/{SSH_USER}/bd/spark/conf/spark-bigdl.conf \"\n",
    "                                                    f\"--jars /home/{SSH_USER}/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar \"\n",
    "                                                    f\"--conf spark.driver.extraClassPath=/home/{SSH_USER}/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar \"\n",
    "                                                    f\"--conf spark.executer.extraClassPath=bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar /home/{SSH_USER}/bd/codes/{row.model} \"\n",
    "                                                    f\"--action train --dataPath /tmp/mnist --batchSize {row.batch_size} --endTriggerNum {row.max_epochs} \"\n",
    "                                                    f\"--learningRate {row.learning_rate} --learningrateDecay 0.0002 > {ex_id}-{filename}\", timeout=(row.max_epochs * 72))\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "        print(stdout.read())\n",
    "        print(stderr.read())\n",
    "    except PipeTimeout:\n",
    "        print(\"PipeTimeout\")\n",
    "        master = MasterNode(driver, f\"master-{ex_id}-1\", master=True)\n",
    "    except socket.timeout:\n",
    "        print(\"Socket timeout\")\n",
    "        master = MasterNode(driver, f\"master-{ex_id}-1\", master=True)\n",
    "   # master.cancel()\n",
    "    sftp = master.ssh.open_sftp()\n",
    "    sftp.get(f'{ex_id}-{filename}', f'raw/{ex_id}/{filename}')\n",
    "\n",
    "    #for slave in slaves:\n",
    "           # slave.stop_failure_worker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in experiments.iterrows():\n",
    "    if row.Index == 160:\n",
    "\n",
    "\n",
    "\n",
    "        #for slave in slaves:\n",
    "        #    slave.start_type()\n",
    "\n",
    "\n",
    "        filename = f\"{int(row.Index)}-nodes8-batch{row.batch_size}-epochs{row.max_epochs}-lrate{row.learning_rate}-memory{row.memory_size}-{row.model}.log\"\n",
    "        sftp = master.ssh.open_sftp()\n",
    "        sftp.get(f'{ex_id}-{filename}', f'raw/{ex_id}/{filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
